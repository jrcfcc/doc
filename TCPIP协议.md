# 							TCP/IP协议

### 协议层

tcp位于整个网络协议的第四层,传输层,它的上一层是ip层，再上一层是数据链路层,每一层都有自己的packet,tcp包只有目标端口和源端口,ip层的packet则记录了目标ip和源ip, tcp/ip包共同定义了一个唯一的链接,每一层都有一个字段表明下层的协议

### TCP标志位详解

新的TCP包头中的标志位有9bits,从低位到高位分别是FIN,SYC,RST,PUSH,ACK,URGENT,ECN,CWR,NONCE

![statu](C:\Users\Administrator\Desktop\statu.jpg)

 

- FIN: "finished"简写。表示发送者以及发送完数据。通常用在发送者发送完数据的最后一个包中。
- SYN: "Synchronisation"简写。表示三次握手建立连接的第一步，在建立连接时发送者发送的第一个包中设置flag值为SYN。
- RST: "reset"简写。重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者发送包发送到一个不是期望的                  目的主机时，接收端发送reset 重置连接标志的包。
- PSH: "push"简写。通知接收端处理接收的报文，而不是将报文缓存到buffer中。
- ACK: "Acknowledgment"简写。表示包已经被成功接收。
- URG: "urgent"简写。通知接收端处理在处理其他包前优先处理接收到的紧急报文（urgent packets）。详见RFC6093。
- ECE: "ECN-Echo"简写。ECN表示Explicit Congestion Notification。表示TCP peer有ECN能力。详见RFC3168。
- CWR: "Congestion Window Reduced"简写。发送者在接收到一个带有ECE flag包时，将会使用CWR flag。 详见RFC3168。
- NS: "nonce sum"简写。该标签用来保护不受发送者发送的突发的恶意隐藏报文的侵害。详见 RFC 3540。



**wireshark抓到的TCP包**

![CDD99711-A301-46dc-A3EC-A956DBDE7A71](C:\Users\Administrator\Desktop\CDD99711-A301-46dc-A3EC-A956DBDE7A71.png)

### TCP的SYN和ACK序列号

**TCP是通过SYN号和ACK号来保证包在传输过程中的有序，ACK号是用来告诉对端,你的下一个包的SYN号值应该是我传给你的SYN号,SYN号是用来让对端确认接收的包是否连续,在TCP传输流中每一个字节对应一个序列号,假设客户端的SYN号为800,同时带有4个字节的数据,那么服务端收到这个包后,就会设置ACK值为804,告诉客户端,我希望收到的下一个包的SYN号为804，服务端也是同理**

### TCP状态及转换过程

**TCP状态流转图**

![stateMachine](C:\Users\Administrator\Desktop\stateMachine.png)

tcp是一个全双工的协议,所以tcp在建立链接的过程中,要互相确认双方的接收和发送能力都正常,链接才能完全建立,同样在断开链接的过程中也要互相确认对方的接收和发送都已经关闭,链接才算完全关闭

#### tcp三次握手过程中的状态及变换过程

- SYN_SENT ：当客户端发起连接请求时,会先初始化一个序列号ISN,长度为32Bits,也就是值在0-2^32之间,请求发送后,此时TCP包中的FLAG标志位中SYN标志为1,假设初始SYN号为800,此包一般被称为SYN包
- SYN_RCVD : 当服务端收到客户端的SYN请求时,服务端确认客户端的发送能力是正常的,服务端也会初始化一个自己的序列号ISN,假设是3000,并设置ACK序列号=客户端的SYN号+1=801,告诉客户端你发送过来的下一个包的SYN序列号的值应该为801,服务端发送的这个tcp包中,FLAG标志位中SYN,和ACK标志为1,此包一般被称为SYN-ACK包
- ESTABLISHED:当客户端收到服务端发送的ACK包后,确认服务端的发送能力和接受能力都是正常的,设置ACK号=服务段的SYN号+1=3001,同时设置自己的SYN号为801,发送给服务端,此时客户端状态为ESTABLISHED,服务端接收到客户端ACK包后,确认客户端的接收能力是正常的,服务端也进入到ESTABLISHED状态

服务端会维持两个队列,一个半连接队列,队列中存储的是处于SYN_SENT状态的链接,全链接队列存储的是处于ESTABLISHED状态的链接

当客户端发送完SYN包后就断开,此时该连接处于服务端的半连接队列中,服务端发送的SYN-ACK包未收到客户端的ACK包确认,会进行5次重试,重试的间隔时间从1秒，2，4，8到16秒,第五次重试后还要等待32秒才能确认第五次也超时了,加起来总共需要63秒来确认客户端SYN超时断开链接,将链接从半连接队列中移除.SYN-FLOOD攻击就是发送大量SYN包给服务端后断开链接,使服务端的半连接队列耗尽。

#### tcp四次挥手过程中的状态及变换过程

- FIN_WAIT_1:主动关闭链接方发送FIN包,此时主动关闭方处于FIN_WAIT_1状态
- CLOSE_WAIT:被动方接收到对端发送的FIN包后,回给对端一个ACK,自身进入CLOSE_WAIT状态
- FIN_WAIT_2:主动关闭方接收到对端发送的ACK包后,进入到FIN_WAIT_2状态
- LAST_ACK:被动关闭方在发送完所有的缓冲数据(也可能没有数据要发送），会发一个FIN包给主动关闭方,此时被动关闭方处于LAST_ACK状态
- TIME_WAIT:主动关闭方接收到对端发送的FIN包后,进入TIME_WAIT状态
- CLOSING:当双方同时发起FIN包时,也就是自身在发送完FIN包后,在接收到对方的ACK包之前,接收到了对方的FIN包，此时就处于CLOSING状态,再收到ACK包后就会进入TIME_WAIT状态
- CLOSED:虚拟的一个状态,用来表示链接已彻底断开

#### **为什么要有TIME_WAIT状态?**

1. 可靠的实现tcp的全双工连接的中止

​    tcp的四次挥手过程中,最终的ACK包是由主动关闭方发出的,如果这个最终的ack包丢失,也就意味着被动关闭方的FIN包没有得到确认,那么被动关闭方就会重新发送一次FIN包,如果主动关闭方在发送完ACK包后没有进入TIME_WAIT状态,而是直接CLOSED关闭链接,那么被动关闭方重发的这个FIN包就无法找到目标的,会收到一个RST分节而不是ASK包,而被动关闭方会把RST分节解析成一个错误(connection reset by peer)

2. 允许老的重复分节在网络中消逝

   TCP的数据包,有可能网络原因阻塞在某个路由器,在阻塞过程中,发送方由于一直没收到ACK确认,会尝试重新发送该数据包,如果这个时候旧的链接主动断开了,并且新起的链接使用的是跟之前一样的端口.在路由修复后,这个之前阻塞的数据包最终可能会抵达新的链接端口。这就可能会造成序列号的混乱。所以主动关闭方引入了一个TIME_WAIT状态。正常情况下,tcp是不允许启用处于TIME_WAIT状态下的端口来建立新的链接的,而从TIME_WAIT状态进入CLOSED的状态需要2MSL时间,而一个IP数据包在网络中存在的最长时间就是MSL(maximum segment lifetime),超过这个时间的数据包会消逝在网络中,从而避免了这种旧的数据包被新的链接接收的问题.

### 超时重传

TCP要保证所有的数据包都可以到达，所以，必须要有重传机制。

当发送方发送了10个字节,4个字节,7个字节,6个字节,12个字节等5个数据包时,如果接收方只接收到了10,4,6,12四个数据包,没有接收到7个字节的数据包时,接收方只会ACK前两个数据包,也就是返回ACK=SYN+1=14+1=15，接收方一直没有收到SYN=15的包,发送方也一直没有收到后续的ACK包,此时就会重传.重传有不同的方式

#### **一 TIMEOUT重传**

​	接收方发送完最后一个连续数据包的ACK=15后,就不再回包,等发送方超时后再次发送SYN=15的数据包.此种情况下,发送方由于一直没收到ACK包,所以也不知道是只有7字节的这个包丢了,还是7,6,12这三个包都丢了,所以发送方重传时,可能只重传7字节的包,也可能7,6,12这三个包都重传

#### 二 快速重传

​    所谓快速重传Fast Retransmit 算法,就是当接收方接收到了SYN=11的包后,返回一个ACK=15的包,后续如果收到了SYN不等于15的包,接收方会一直返回ACK=15,当发送方连续三次收到ACK=15的包后,就知道是第三个SYN=15的7个字节的包丢了,需要重传,接收方收到第三个包后,此时就可以直接回一个ACK=40，因为后续的包已经收到了。这种就不需要一直等到发送方TIMEOUT才重传

#### 三 SACK重传

​    还有一种更新进的方式叫做SACK重传,Selective Acknowledgment (SACK)（参看RFC 2018）。就是在TCP头中加入SACK字段。ACK的算法还是Fast Retransmit ,只是SACK回汇报接收方已经接收到的数据包碎片,告诉发送方有哪些数据包丢失了,如下图,接收方接收到了一个SYN=100,199字节的数据包先回一个300的ACK,但是SYN=300,199字节的这个数据包丢失了,接收方接收到了SYN=500,199字节的数据包,此时接收方会回一个ACK=300,SACK 500-700的包给发送方,发送方就可以知道是SYN=300,199字节的这个包丢失了,需要重传。接收方又接收到了一个SYN=900,199字节的包时,由于还没有收到SYN=300的包,所以回包的ACK还是=300,SACK则是900-1100，500-700.

![SACK](C:\Users\Administrator\Desktop\SACK.jpg)

在 Linux下，可以通过tcp_sack参数打开这个功能（Linux 2.4后默认打开）。

#### 重复发送

之前说的都是丢包重传,还有一种场景是重复发送,这个对应的处理算法就是,Duplicate SACK又称D-SACK

重复发送的原因就是因为接收方回的ACK包没有被发送方收到,导致发送方重复发送,D-SACK就是使用SACK来告诉发送方,哪些包重复接收了。怎么确认是D-SACK呢,就是如果SACK的第一个段的范围被ACK所覆盖了,就是D-SACK,或者SACK的第二个段被第一个段覆盖了,就是D-SACK

- ACK丢包

  比如发送方发了200-299,300-599,600-800三个包,接收方也回了三个ACK包,但是后面两个ACK包丢失了,发送方又发送了一遍,300-599,600-800两个包,这时接收方收到后,就会返回ACK=801,SACK=300-599,600-800,SACK的值小于ACK,说明是D-SACK，发送方就知道是接收方的ACK包丢包了,而不是自己发送的包丢了

- 网络延时 (**这段直接copy自[通俗易懂]深入理解TCP协议（上）：理论基础**) 

  下面的示例中，网络包（1000-1499）被网络给延误了，导致发送方没有收到ACK，而后面到达的三个包触发了“Fast Retransmit算法”，所以重传，但重传时，被延误的包又到了，所以，回了一个SACK=1000-1500，因为ACK已到了3000，所以，这个SACK是D-SACK——标识收到了重复的包。这个案例下，发送端知道之前因为“Fast Retransmit算法”触发的重传不是因为发出去的包丢了，也不是因为回应的ACK包丢了，而是因为网络延时了。

     ```shell
Transmitted    Received    ACK Sent
Segment        Segment     (Including SACK Blocks)
  
500-999        500-999     1000
1000-1499      (delayed)
1500-1999      1500-1999   1000, SACK=1500-2000
2000-2499      2000-2499   1000, SACK=1500-2500
2500-2999      2500-2999   1000, SACK=1500-3000
1000-1499      1000-1499   3000
                       1000-1499   3000, SACK=1000-1500
                                      ---------
     ```



**引入了D-SACK，有这么几个好处：**

- 1）可以让发送方知道，是发出去的包丢了，还是回来的ACK包丢了。
- 2）是不是自己的timeout太小了，导致重传。
- 3）网络上出现了先发的包后到的情况（又称reordering）
- 4）网络上是不是把我的数据包给复制了。

### 滑动窗口算法

TCP必需要解决的可靠传输以及包乱序（reordering）的问题，所以，TCP必需要知道网络实际的数据处理带宽或是数据处理速度，这样才不会引起网络拥塞，导致丢包。

#### 一 概述

所以，TCP引入了一些技术和设计来做网络流控，Sliding Window是其中一个技术。 前面我们说过，TCP头里有一个字段叫Window，又叫Advertised-Window，这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。 为了说明滑动窗口，我们需要先看一下TCP缓冲区的一些数据结构：

![6199622-9065b53336c8d9a0](C:\Users\Administrator\Desktop\6199622-9065b53336c8d9a0.webp)

**发送端**:LastByteAcked指向最大的被接收端ACK确认收到的包,在这个位置之前的包可以从TCP缓冲区中删除,LastByteSent表示已经发出去,但是还没有被接收到确认收到的包,未发送可发送的区域就是应用程序已经写到缓冲区中但是还没有发送的包

![6199622-d45800849783e47e](C:\Users\Administrator\Desktop\6199622-d45800849783e47e.webp)

1. 第一部分：接收并且确认过的
2. 还没有接收但是马上就能接收的，也是我能够接受的最大工作量
3. 还没有接收，也没有办法接收的，超过工作量的部分

- MaxRcvBuffer 最大缓存的量
- LastByteRead之后是已经接收了的，但是还没有被应用层读取的
- NextByteExcepted是第一部分和第二部分的分界线

第二部分的窗口有多大？

NextByteExpected 和 LastByteRead的差其实是还没有被应用层读取的部分占用调MaxRcvBuffer的量，定义为A，**窗口大小其实是MaxRcvBuffer减去A**

其中第二部分里面，由于收到的包可能不是顺序的，会出现空档，**只有和第一部分连续的，可以马上进行回复**，中间空着的部分需要等待，哪怕后面的已经来了(可以看到接收端的窗口出现了虚线和实线的区别)

**于是：**

- 接收端在给发送端回ACK中会汇报自己的AdvertisedWindow = MaxRcvBuffer – (NextByteExpected – LastByteRead);
- 而发送方会根据这个窗口来控制发送数据的大小，以保证接收方可以处理。


**下面我们来看一下发送方的滑动窗口示意图：**

![[通俗易懂]深入理解TCP协议（下）：RTT、滑动窗口、拥塞处理_3.png](http://www.52im.net/data/attachment/forum/201609/03/101120ihanzfsth0hhcllk.png)



**上图中分成了四个部分，分别是：（其中那个黑模型就是滑动窗口）**



- \#1 已收到ack确认的数据。
- \#2 发还没收到ack的。
- \#3 在窗口中还没有发出的（接收方还有空间）。
- \#4 窗口以外的数据（接收方没空间）


**下面是个滑动后的示意图（收到36的ack，并发出了46-51的字节）：**

![[通俗易懂]深入理解TCP协议（下）：RTT、滑动窗口、拥塞处理_1.png](http://www.52im.net/data/attachment/forum/201609/03/104938yhzgro73pypszrop.png)



**下面我们来看一个接受端控制发送端的图示：**

![[通俗易懂]深入理解TCP协议（下）：RTT、滑动窗口、拥塞处理_2.png](http://www.52im.net/data/attachment/forum/201609/03/104946z1skgjjxx8g0txbx.png)





#### 二 Zero Window


上图，我们可以看到一个处理缓慢的Server（接收端）是怎么把Client（发送端）的TCP Sliding Window给降成0的。此时，你一定会问，如果Window变成0了，TCP会怎么样？是不是发送端就不发数据了？是的，发送端就不发数据了，你可以想像成“Window Closed”，那你一定还会问，如果发送端不发数据了，接收方一会儿Window size 可用了，怎么通知发送端呢？

解决这个问题，TCP使用了Zero Window Probe技术，缩写为ZWP，也就是说，发送端在窗口变成0后，会发ZWP的包给接收方，让接收方来ack他的Window尺寸，一般这个值会设置成3次，第次大约30-60秒（不同的实现可能会不一样）。如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。

**注意：**只要有等待的地方都可能出现DDoS攻击，Zero Window也不例外，一些攻击者会在和HTTP建好链发完GET请求后，就把Window设置为0，然后服务端就只能等待进行ZWP，于是攻击者会并发大量的这样的请求，把服务器端的资源耗尽。（关于这方面的攻击，大家可以移步看一下[Wikipedia的SockStress词条](http://en.wikipedia.org/wiki/Sockstress)）

另外，Wireshark中，你可以使用tcp.analysis.zero_window来过滤包，然后使用右键菜单里的follow TCP stream，你可以看到ZeroWindowProbe及ZeroWindowProbeAck的包。



#### 三 Silly Window Syndrome


Silly Window Syndrome翻译成中文就是“糊涂窗口综合症”。正如你上面看到的一样，如果我们的接收方太忙了，来不及取走Receive Windows里的数据，那么，就会导致发送方越来越小。到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的window，而我们的发送方会义无反顾地发送这几个字节。

要知道，我们的TCP+IP头有40个字节，为了几个字节，要达上这么大的开销，这太不经济了。

另外，你需要知道网络上有个MTU，对于以太网来说，MTU是1500字节，除去TCP+IP头的40个字节，真正的数据传输可以有1460，这就是所谓的MSS（Max Segment Size）注意，TCP的RFC定义这个MSS的默认值是536，这是因为 RFC 791里说了任何一个IP设备都得最少接收576尺寸的大小（实际上来说576是拨号的网络的MTU，而576减去IP头的20个字节就是536）。

如果你的网络包可以塞满MTU，那么你可以用满整个带宽，如果不能，那么你就会浪费带宽。（大于MTU的包有两种结局，一种是直接被丢了，另一种是会被重新分块打包发送） 你可以想像成一个MTU就相当于一个飞机的最多可以装的人，如果这飞机里满载的话，带宽最高，如果一个飞机只运一个人的话，无疑成本增加了，也而相当二。

所以，Silly Windows Syndrome这个现像就像是你本来可以坐200人的飞机里只做了一两个人。 要解决这个问题也不难，就是避免对小的window size做出响应，直到有足够大的window size再响应，这个思路可以同时实现在sender和receiver两端：



- 如果这个问题是由Receiver端引起的，那么就会使用 David D Clark’s 方案。在receiver端，如果收到的数据导致window size小于某个值，可以直接ack(0)回sender，这样就把window给关闭了，也阻止了sender再发数据过来，等到receiver端处理了一些数据后windows size 大于等于了MSS，或者，receiver buffer有一半为空，就可以把window打开让send 发送数据过来。
- 如果这个问题是由Sender端引起的，那么就会使用著名的 [Nagle’s algorithm](http://en.wikipedia.org/wiki/Nagle's_algorithm)。这个算法的思路也是延时处理，他有两个主要的条件：1）要等到 Window Size>=MSS 或是 Data Size >=MSS，2）收到之前发送数据的ack回包，他才会发数据，否则就是在攒数据。


另外，Nagle算法默认是打开的，所以，对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性比较强的程序，你需要关闭这个算法。你可以在Socket设置TCP_NODELAY选项来关闭这个算法（关闭Nagle算法没有全局参数，需要根据每个应用自己的特点来关闭）

```
setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (``char` `*)&value,``sizeof``(``int``));
```

另外，网上有些文章说TCP_CORK的socket option是也关闭Nagle算法，这不对。TCP_CORK其实是更新激进的Nagle算汉，完全禁止小包发送，而Nagle算法没有禁止小包发送，只是禁止了大量的小包发送。最好不要两个选项都设置。

#### 拥塞控制算法


上面我们知道了，TCP通过Sliding Window来做流控（Flow Control），但是TCP觉得这还不够，因为Sliding Window需要依赖于连接的发送端和接收端，其并不知道网络中间发生了什么。TCP的设计者觉得，一个伟大而牛逼的协议仅仅做到流控并不够，因为流控只是网络模型4层以上的事，TCP的还应该更聪明地知道整个网络上的事。

具体一点，我们知道TCP通过一个timer采样了RTT并计算RTO，但是，如果网络上的延时突然增加，那么，TCP对这个事做出的应对只有重传数据，但是，重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，于是，这个情况就会进入恶性循环被不断地放大。试想一下，如果一个网络内有成千上万的TCP连接都这么行事，那么马上就会形成“网络风暴”，TCP这个协议就会拖垮整个网络。这是一个灾难。

所以，TCP不能忽略网络上发生的事情，而无脑地一个劲地重发数据，对网络造成更大的伤害。对此TCP的设计理念是：TCP不是一个自私的协议，当拥塞发生的时候，要做自我牺牲。就像交通阻塞一样，每个车都应该把路让出来，而不要再去抢路了。（关于拥塞控制的论文请参看《[Congestion Avoidance and Control](http://ee.lbl.gov/papers/congavoid.pdf)》(PDF)）

RTT——Round Trip Time，也就是一个数据包从发出去到回来的时间

部分名词解释:

- MTU:由于以太网EthernetII最大的数据帧是1518Bytes这样，刨去以太网帧的帧头（DMAC目的地址MAC48bit=6Bytes+SMAC源MAC地址48bit=6Bytes+Type域2bytes）14Bytes和帧尾CRC校验部分4Bytes（这个部门有时候大家也把它叫做FCS），那么剩下承载上层协议的地方也就是Data域最大就只能有1500Bytes. 这个值我们就把它称之为MTU。
- MSS:MSS就是TCP数据包每次能够传输的最大数据分段。为了达到最佳的传输效能,TCP协议在建立连接的时候通常要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以往往MSS为1460。通讯双方会根据双方提供的MSS值得最小值确定为这次连接的最大MSS值。



拥塞控制算法主要有四个算法,慢启动,拥塞避免,拥塞控制,快速恢复

1. **慢启动(Slow Start)**:由小到大慢慢增加拥塞窗口CWND的大小,一般CWND初始为一个MSS,也就是一次只能发一个包,每收到一个ACK,CWND就+1,比如,第一个发了一个包,收到ACK后,CWND=2，下次就可以发送两个包,收到接收方回的两个ACK后,此时的CWND就等于2+2=4,相当于每过一个RTT，CWND的值就会翻倍,也就是说在慢启动阶段,CWND为指数式增长,见下图

   ![20170701100121964](C:\Users\Administrator\Desktop\20170701100121964.png)

   同时慢启动算法有一个上限值ssthresh(slow start threshold),一般来说ssthresh的值是65535个字节,当CWND>=ssthresh的时候,就跨过慢启动过程进入了拥塞避免阶段.

2. **拥塞避免**:

   当进入拥塞避免阶段后,CWND的值在每收到一次ACK后,CWND的值就增加1/CWND，假设当前的CWND下,一次能发送5个数据包,那么在一轮RTT后,CWND的值为5 + 5 * （1/5）= 6,也就是CWND在拥塞避免阶段是线性增长的,也就是所谓的AI(加性增窗)

3. **拥塞控制**

   当链接出现丢包的时候,TCP就回认为当前链接已经进入了拥塞状态,丢包有以下两种情况

   - RTO超时，重传数据包：这种情况下,tcp会将ssthresh的值设置为CWND的一半CWND/2,同时将CWND重置为1
   - Fast Retransmit算法，也就是在收到3个duplicate ACK时就开启重传，而不用等到RTO超时：这种情况下TCP会将CWND设置为之前的一半,然后将ssthresh的值设置跟CWND相等,同时进入快速恢复算法

4. **快速恢复**

   快速重传和快速恢复算法一般同时使用。快速恢复算法是认为，你还有3个Duplicated Acks说明网络也不那么糟糕，所以没有必要像RTO超时那么强烈。 注意，正如前面所说，进入Fast Recovery之前，cwnd 和 sshthresh已被更新：

   

   - cwnd = cwnd /2
   - sshthresh = cwnd


   **然后，真正的Fast Recovery算法如下：**

   

   - cwnd = sshthresh + 3 * MSS （3的意思是确认有3个数据包被收到了）
   - 重传Duplicated ACKs指定的数据包
   - 如果再收到 duplicated Acks，那么cwnd = cwnd +1
   - 如果收到了新的Ack，那么，cwnd = sshthresh ，然后就进入了拥塞避免的算法了。



用一张图来感受这四种算法对应的CWND走势

![110456fk5xtvh9gltkllzl](C:\Users\Administrator\Desktop\110456fk5xtvh9gltkllzl.jpg)

#### 拥塞控制算法之BBR算法

传统的拥塞控制算法都是通过丢包来判断网络是否达到了拥塞状态,通过调整CWND大小来控制tcp的发包数量,但是没有对发送速率进行限制,也就是说如果当前CWND允许发送10个包,那么传统算法会一次性把这10个包全发出去,也就是突发。实际上整个tcp的发包都属于突发,能发多少我就一次全发出去。但是BBR算法是通过PACE RATING来控制发包速率的,也就是说如果CWND允许我发10个包,但是我不会一次全发出去,而是根据当前的速率来依次发送这10个包,同时BBR也不跟丢包,也就是当网络中发生丢包的时候,BBR算法不认为就一定是网络中发生了拥塞,而是通过当前的即时带宽来判断是否发生立刻拥塞.下面就讲讲BBR算法中的五个关键点.

1. 即时速率的计算

   计算一个即时的带宽bw，该带宽是bbr一切计算的基准，bbr将会根据当前的即时带宽以及其所处的pipe状态来计算pacing rate以及cwnd(见下文)，后面我们会看到，这个即时带宽计算方法的突破式改进是bbr之所以简单且高效的根源。计算方案按照标量计算，不再关注数据的含义。在bbr运行过程中，系统会跟踪当前为止最大的即时带宽。

2. RTT的跟踪

   bbr之所以可以获取非常高的带宽利用率，是因为它可以非常安全且豪放地探测到带宽的最大值以及rtt的最小值，这样计算出来的BDP就是目前为止TCP管道的最大容量。bbr的目标就是达到这个最大的容量！这个目标最终驱动了cwnd的计算。在bbr运行过程中，系统会跟踪当前为止最小RTT。

3. BBR PIPE状态机的维持

   bbr算法根据互联网的拥塞行为有针对性地定义了4中状态，即STARTUP，DRAIN，PROBE_BW，PROBE_RTT。bbr通过对上述计算的即时带宽bw以及rtt的持续观察，在这4个状态之间自由切换，相比之前的所有拥塞控制算法，其革命性的改进在于bbr拥塞算法不再跟踪系统的TCP拥塞状态机，而旨在用统一的方式来应对pacing rate和cwnd的计算，不管当前TCP是处在Open状态还是处在Disorder状态，抑或已经在Recovery状态，换句话说，bbr算法感觉不到丢包，它能看到的就是bw和rtt！

4. 拥塞判断标尺和调整结果PACING RATE和CWND

   首先必须要说一下，bbr的输出并不仅仅是一个cwnd，更重要的是pacing rate。在传统意义上，cwnd是TCP拥塞控制算法的唯一输出，但是它仅仅规定了当前的TCP最多可以发送多少数据，它并没有规定怎么把这么多数据发出去，在Linux的实现中，如果发出去这么多数据呢？简单而粗暴，突发！忽略接收端通告窗口的前提下，Linux会把cwnd一窗数据全部突发出去，而这往往会造成路由器的排队，在深队列的情况下，会测量出rtt剧烈地抖动。bbr在计算cwnd的同时，还计算了一个与之适配的pacing rate，该pacing rate规定cwnd指示的一窗数据的数据包之间，以多大的时间间隔发送出去。

5. 其它外部机制的利用

   bbr之所以可以高效地运行且如此简单，是因为很多机制并不是它本身实现的，而是利用了外部的已有机制，比如下一节中将要阐述的它为什么在计算带宽bw时能如此放心地将重传数据也计算在内...

#### 带宽计算细节以及状态机

##### 1.即时带宽的计算

bbr作为一个纯粹的拥塞控制算法，完全忽略了系统层面的TCP状态，计算带宽时它仅仅需要两个值就够了：
**1).应答了多少数据，记为delivered；**
**2).应答1)中的delivered这么多数据所用的时间，记为interval_us。**
将上述二者相除，就能得到带宽：
**bw = delivered/interval_us**
非常简单！以上的计算完全是标量计算，只关注数据的大小，不关注数据的含义，比如delivered的采集中，bbr根本不管某一个应答是重传后的ACK确认的，正常ACK确认的，还是说SACK确认的。bbr只关心被应答了多少！
    这和TCP/IP网络模型是一致的，因为在中间链路上，路由器交换机们也不会去管这些数据包是重传的还是乱序的，然而拥塞也是在这些地方发生的，既然拥塞点都不关心数据的意义，TCP为什么要关注呢？反过来，我们看一下拥塞发生的原因，即数据量超过了路由器的带宽限制，利用这一点，只需要精心地控制发送的数据量就好了，完全不用管什么乱序，重传之类的。当然我的意思是说，拥塞控制算法中不用管这些，但这并不意味着它们是被放弃的，其它的机制会关注的，比如SACK机制，RACK机制，RTO机制等。
    接下来我们看一下这个delivered以及interval_us的采集是如何实现的。还是像往常一样，我不准备分析源码，因为如果分析源码的话，往往难以抓住重点，过一段时间自己也看不懂了，相反，画图的话，就可以过滤掉很多诸如unlikely等异常流或者当前无需关注的东西：



![img](https://img-blog.csdn.net/20161016152834678?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



上图中，我故意用了一个极端点的例子，在该例子中，我几乎都是使用的SACK，当X被SACK时，我们可以根据图示很容易算出从Delivered为7时的数据包被确认到X被确认为止，一共有**12-7=5**个数据包被确认，即这段时间网络上清空了5个数据包！我们便很容易算出带宽值了。我的这个图示在解释带宽计算方法之外，还有一个目的，即说明bbr在计算带宽时是不关注数据包是否按序确认的，它只关注数量，即数据包被网络清空的数量。实实在在的计算，不猜Lost，不猜乱序，这些东西，你再怎么猜也猜不准！
    计算所得的bw就是bbr此后一切计算的基准。

##### 2.状态机

bbr的状态机转换图以及注释如下图所示：



![img](https://img-blog.csdn.net/20161016152851585?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



通过上述的状态机以及上一节的带宽计算方式，我们知道了bbr的工作方式：不断地基于当前带宽以及当前的增益系数计算pacing rate以及cwnd，以此2个结果作为拥塞控制算法的输出，在TCP连接的持续过程中，每收到一个ACK，都会计算即时的带宽，然后将结果反馈给bbr的pipe状态机，不断地调节增益系数，这就是bbr的全部，我们发现它是一个典型的封闭反馈系统，与TCP当前处于什么拥塞状态完全无关，其简图如下：



![img](https://img-blog.csdn.net/20161016152908882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



这非常不同于之前的所有拥塞控制算法，在之前的算法中，我们发现拥塞算法内部是受外部的拥塞状态影响的，比如说在Recovery状态下，甚至都不会进入拥塞控制算法，在bbr进入内核之前，Linux使用PRR算法控制了Recovery状态的窗口调整，即便说这个时候网络已经恢复，TCP也无法发现，因为TCP的Recovery状态还未恢复到Open，这就是根源！

#### pacing rate以及cwnd的计算

这一节好像是重点中的重点，但是我觉得如果理解了bbr的带宽计算，状态机以及其增益系数的概念，这里就不是重点了，这里只是一个公式化的结论。
    pacing rate怎么计算？很简单，就是是使用时间窗口内(默认10轮采样)最大BW。上一次采样的即时BW，用它来在可能的情况下更新时间窗口内的BW采样值集合。这次能否按照这个时间窗口内最大BW发送数据呢？这样看当前的增益系数的值，设为G，那么BW*G就是pacing rate的值，是不是很简单呢？！
    至于说cwnd的计算可能要稍微复杂一点，但是也是可以理解的，我们知道，cwnd其实描述了一条网络管道(rwnd描述了接收端缓冲区)，因此cwnd其实就是这个管道的容量，也就是BDP！
    BW我们已经有了，缺少的是D，也就是RTT，不过别忘了，bbr一直在持续搜集最小的RTT值，注意，bbr并没有采用什么移动指数平均算法来“猜测”RTT(我用猜测而不是预测的原因是，猜测的结果往往更加不可信！)，而是直接冒泡采集最小的RTT(注意这个RTT是TCP系统层面移动指数平均的结果，即SRTT，但brr并不会对此结果再次做平均！)。我们用这个最小RTT干什么呢？
    当前是计算BDP了！这里bbr取的RTT就是这个最小RTT。最小RTT表示一个曾经达到的最佳RTT，既然曾经达到过，说明这是客观的可以再次达到的RTT，这样有益于网络管道利用率最大化！
    我们采用BDP*G'就算出了cwnd，这里的G'是cwnd的增益系数，与带宽增益系数含义一样，根据bbr的状态机来获取！

#### bbr的细节浅述

该节的题目比较怪异，既然是细节为什么又要浅述？？
    这是我的风格，一方面，说是细节是因为这些东西还真的很少有人注意到，另一方面，说是浅述，是因为我一般都不会去分析代码以及代码里每一个异常流，我认为那些对于理解原理帮助不大，那些东西只是在研发和优化时才是有用的，所以说，像往常一样，我这里的这个小节还是一如既往地去谈及一些“细节”。

##### 1.豪放且大胆的安全探测

在看到bbr之后，我觉得之前的TCP拥塞控制算法都错了，并不是思想错了，而是实现的问题。
bbr之所以敢大胆的去探测预估带宽是因为TCP把更多的权力交给了它！在bbr之前，很多本应该由拥塞控制算法去处理的细节并不归拥塞控制算法管。在详述之前，我们必须分清两件事：
1).传输多少数据？
2).传输哪些数据？
按照“上帝的事情上帝管，凯撒的事情凯撒管”的原则，这两件事本来就该由不同的机制来完成，不考虑对端接收窗口的情况下，拥塞窗口是唯一的主导因素，“传输多少数据”这件事应该由拥塞算法来回答，而“传输哪些数据”这个问题应该由TCP拥塞状态机以及SACK分布来决定，诚然这两个问题是不同的问题，不应该杂糅在一起。
    然而，在bbr进入内核之前的Linux TCP实现中，以上两个问题并不是分得特别清。TCP的拥塞状态只有在Open时才是上述的职责分离的完美样子，一旦进入Lost或者Recovery，那么拥塞控制算法即便对“问题1)：传输多少数据”都无能为力，在Linux的现有实现中，PRR算法将接管一切，一直把窗口下降到ssthresh，在Lost状态则反应更加激烈，直接cwnd硬着陆！随后等丢失数据传输成功后再执行慢启动....在重新进入Open状态之前，拥塞控制算法几乎不会起作用，这并不是一种高速公路上的模式(小碰擦，拍照后停靠路边，自行解决)，更像是闹市区的交通事故处理方式(无论怎样，保持现场，直到交警和保险公司的人来现场处置)。



参考资料:

[1]: https://blog.csdn.net/gyunling/article/details/89330307	"TCP 报文格式及TCP Flags"
[2]: https://www.jianshu.com/p/ff26312e67a9	"tcp的半连接与完全连接队列"
[3]: https://blog.csdn.net/swanabin/article/details/59114153	"理解TCP序列号（Sequence Number）和确认号（Acknowledgment Number）"
[4]: http://www.52im.net/thread-513-1-1.html	"[通俗易懂]深入理解TCP协议（上）：理论基础"
[5]: https://my.oschina.net/u/3421984/blog/1618271	"关于tcp中time_wait状态的4个问题"
[6]: http://www.52im.net/thread-515-1-1.html	"[通俗易懂]深入理解TCP协议（下）：RTT、滑动窗口、拥塞处理"
[7]: https://www.jianshu.com/p/d23c0d4eba12	"TCP和UDP的区别"
[8]: https://blog.csdn.net/u013929635/article/details/80275167	"来自Google的TCP BBR拥塞控制算法解析"

